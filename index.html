<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HTML: Hybrid Temporal-scale Multimodal Learning framework for Referring Video Object Segmentation.">
  <meta name="keywords" content="HTML, RVOS, Multimodal Learning, Temporal-scale">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HTML: Hybrid Temporal-scale Multimodal Learning framework for Referring Video Object Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://mingfei.info">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HTML: Hybrid Temporal-scale Multimodal Learning framework for Referring Video Object Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingfei.info/">Mingfei Han</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ/">Yali Wang</a><sup>&dagger;</sup>,
            </span>
            <span class="author-block">Zhihui Li,
            </span>
            <span class="author-block">
              <a href="https://www.linayao.com/">Lina Yao</a>,
            </span>
            <span class="author-block">
              <a href="https://www.xiaojun.ai/">Xiaojun Chang</a>,
            </span>
            <span class="author-block">
              <a href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a>
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://mingfei.info/files/ICCV_RVOSv2.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Referring Video Object Segmentation (RVOS) is to segment the object instance from a given video, according to the textual description of this object. However, in the open world, the object descriptions are often diversified in contents and flexible in lengths. This leads to the key difficulty in RVOS, i.e., various descriptions of different objects are corresponding to different temporal scales in the video, which is ignored by most existing approaches with single stride offrame sampling. To tackle this problem, we propose a concise Hybrid Temporal-scale Multimodal Learning (HTML) framework, which can effectively align lingual and visual features to discover core object semantics in the video, by learning multimodal interaction hierarchically from different temporal scales. More specifically, we introduce a novel inter-scale multimodal perception module, where the language queries dynamically interact with visual features across temporal scales. It can effectively reduce complex object confusion by passing video context among different scales. Finally, we conduct extensive experiments on the widely used benchmarks, including Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences, where our HTML achieves state-of-the-art performance on all these datasets.
        </p>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->

  <!-- Contributions. -->
  <div class="columns is-centered has-text-centered">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Highlights</h2>
        
        <div class="content has-text-justified">
          <ul>
            <li>Our method with ResNet-50 achieves 57.8 in L&F, surpassing the recent SOTA with ResNet-101.</li>
            <li>Our HTML boosts the baseline model without additional modules and computations during inference.</li>
            <li>Our HTML can significantly benefit from diversified text descriptions, as shown in Figure 1.</li>
          </ul>
        </div>
          <div class="wrap" align="center">
            <img src="./static/images/benefit_texts.png" 
                 alt="Benefiting from diversified language descriptions."
                 style="width:75%;height:auto;text-align:center;">
          </div>
          <div class="caption" align="center">
            <br/>
            <p class="caption-content">
            L&F comparison on using language descriptions in different lengths.
            </p>
          </div>
        <br/>
        <!--/ Interpolating. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
  <!--/ Contributions. -->

  <!-- Contributions. -->
  <!-- <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Contributions</h2>
      <div class="content has-text-justified">
        <ul>
          <li>Concise and unified learning framework: our Hybrid Temporal-scale Multimodal Learning (HTML) framework hierarchically constructs multimodal interactions via different strides of frame sampling, which can mutually enhance embeddings from both modalities for accurate segmentation.</li>
          <li>Effective multimodal perception module: our Cross-scale Multimodal Perception (CMP) module can effectively reduce complex object confusions with intra-scale and inter-scale multimodal perceptions, where linguistic and visual features interact across temporal scales.</li>
          <li>State-of-the-art performance on the widely-used benchmarks, which shows the superiority of our framework. Specifically, on Ref-Youtube-VOS [22], our method with ResNet-50 achieves 57.8 in L&F, outperforming the recent SOTA method [29] with ResNet-101.</li>
        </ul>
      </div>
    </div>
  </div> -->
  <!--/ Contributions. -->

  <!-- Example Cases and Experiment Results Figures Placeholder -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Experiment Results</h2>
      <div class="content has-text-justified">
        <!-- Insert your figures and tables here -->
      </div>
    </div>
  </div>
  <!--/ Example Cases and Experiment Results Figures Placeholder -->

</div>
</section>

<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>
    @inproceedings{han2023html,
      title={HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation},
      author={Han, Mingfei and Wang, Yali and Li, Zhihui and Yao, Lina and Chang, Xiaojun and Qiao, Yu},
      booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
      year={2023},
      organization={IEEE}
    }
  </code></pre>
</div>
</section>


<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          Website design from <a
          href="https://nerfies.github.io/">Nerfies</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>

</body>
</html>
